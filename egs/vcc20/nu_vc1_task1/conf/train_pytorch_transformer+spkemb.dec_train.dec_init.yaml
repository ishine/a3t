accum-grad: 1
adim: 384
aheads: 4
batch-bins: 500000
batch-sort-key: output
bce-pos-weight: 10.0
dec-init: ../../libritts/tts1/exp/train_clean_460_pytorch_ept_baseline/ept_results/snapshot.ep.100
decoder-concat-after: false
decoder-normalize-before: false
dlayers: 6
dprenet-dropout-rate: 0.5
dprenet-layers: 2
dprenet-units: 256
dunits: 1536
elayers: 6
embed-dim: 0
enc-init: ../../libritts/tts1/exp/train_clean_460_pytorch_ept_baseline/ept_results/snapshot.ep.100
enc-init-mods: encoder
encoder-concat-after: false
encoder-normalize-before: false
epochs: 2000
eprenet-conv-chans: 0
eprenet-conv-filts: 0
eprenet-conv-layers: 0
eprenet-dropout-rate: 0.0
eunits: 1536
eval-interval-epoch: 100
freeze-mods: encoder
grad-clip: 1.0
initial-decoder-alpha: 1.0
initial-encoder-alpha: 1.0
lr: 0.001
model-module: espnet.nets.pytorch_backend.vc_transformer:Transformer
opt: lamb
patience: 0
postnet-chans: 256
postnet-dropout-rate: 0.5
postnet-filts: 5
postnet-layers: 5
reduction-factor: 2
save-interval-epoch: 100
spk-embed-integration-type: add
transformer-dec-attn-dropout-rate: 0.1
transformer-dec-dropout-rate: 0.1
transformer-dec-positional-dropout-rate: 0.1
transformer-enc-attn-dropout-rate: 0.1
transformer-enc-dec-attn-dropout-rate: 0.1
transformer-enc-dropout-rate: 0.1
transformer-enc-positional-dropout-rate: 0.1
transformer-init: pytorch
transformer-input-layer: conv2d-scaled-pos-enc
transformer-lr: 1.0
transformer-warmup-steps: 4000
use-batch-norm: true
use-guided-attn-loss: false
use-masking: true
use-scaled-pos-enc: true
use-speaker-embedding: true
weight-decay: 0.0
